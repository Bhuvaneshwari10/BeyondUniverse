{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is the process of automatically extracting information from websites. It involves fetching web pages, parsing the HTML or XML content of those pages, and extracting the desired information. This extracted information can then be saved into a structured format such as a CSV file or a database for further analysis or use.\n",
    "\n",
    "Here are the key steps involved in web scraping:\n",
    "\n",
    "    Fetching Web Pages: The first step in web scraping is to send an HTTP request to the target website's server to fetch the web page's HTML content. This can be done using libraries like Requests in Python.\n",
    "\n",
    "    Parsing HTML/XML: Once the web page content is fetched, the next step is to parse the HTML or XML content to extract the desired data. Libraries such as BeautifulSoup (for HTML parsing) or lxml can be used for this purpose.\n",
    "\n",
    "    Extracting Data: After parsing the HTML/XML content, web scrapers can extract specific data elements such as text, tables, images, links, or other structured information based on their location within the HTML structure.\n",
    "\n",
    "    Cleaning and Structuring Data: The extracted data may need cleaning and preprocessing to remove unwanted characters, format dates, or handle missing values. After cleaning, the data can be structured into a suitable format like JSON, CSV, or saved directly to a database.\n",
    "\n",
    "    Automation: Web scraping can be automated to crawl multiple pages or websites to extract a large amount of data. However, it's important to follow ethical guidelines, respect website terms of service, and avoid overloading servers with too many requests (which can lead to IP blocking or legal issues).\n",
    "\n",
    "Web scraping is commonly used for various purposes such as gathering data for research, market analysis, competitive intelligence, price monitoring, news aggregation, and more. However, it's crucial to be mindful of legal and ethical considerations, respect website terms of use, and obtain permission if necessary before scraping data from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is beautifulsoap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for web scraping tasks, especially for parsing HTML and XML documents. It provides convenient methods and functions to navigate and extract data from HTML/XML content. Below are the key steps for working with Beautiful Soup:\n",
    "\n",
    "    Install Beautiful Soup:\n",
    "    Before using Beautiful Soup, you need to install it. You can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Web Page Content:\n",
    "Use the requests library to fetch the HTML content of the web page you want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fetching the content of a webpage\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Beautiful Soup Object:\n",
    "Create a Beautiful Soup object by passing the HTML content and a parser (such as 'html.parser')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the HTML Tree:\n",
    "Beautiful Soup provides methods to navigate the HTML/XML tree and find specific elements (tags, attributes, text, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "# Example: Finding all <a> tags (hyperlinks) in the HTML content\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link.get('href'))  # Print the href attribute of each <a> tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data:\n",
    "Use Beautiful Soup methods like find, find_all, and CSS selectors to extract specific data from the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracting text from specific elements\n",
    "title = soup.find('title').text\n",
    "paragraph = soup.find('p').text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Nested Elements:\n",
    "Beautiful Soup allows navigating through nested elements and extracting data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Navigating through nested elements\u001b[39;00m\n\u001b[0;32m      2\u001b[0m parent_element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m child_element \u001b[38;5;241m=\u001b[39m \u001b[43mparent_element\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Example: Navigating through nested elements\n",
    "parent_element = soup.find('div', class_='parent')\n",
    "child_element = parent_element.find('p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent element not found.\n"
     ]
    }
   ],
   "source": [
    "parent_element = soup.find('div', class_='parent')\n",
    "if parent_element:\n",
    "    child_element = parent_element.find('p')\n",
    "    if child_element:\n",
    "        # Do something with child_element\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Child <p> element not found.\")\n",
    "else:\n",
    "    print(\"Parent element not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent element not found.\n"
     ]
    }
   ],
   "source": [
    "parent_element = soup.find('div', class_='parent')\n",
    "if parent_element:\n",
    "    child_element = parent_element.find('p')\n",
    "    if child_element:\n",
    "        # Do something with child_element\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Child <p> element not found.\")\n",
    "else:\n",
    "    print(\"Parent element not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Parsing:\n",
    "Beautiful Soup supports advanced parsing techniques such as parsing XML, handling malformed HTML, and specifying different parsers (like lxml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xml_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Parsing XML content\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m soup_xml \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[43mxml_content\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xml_content' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: Parsing XML content\n",
    "soup_xml = BeautifulSoup(xml_content, 'xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling:\n",
    "Handle exceptions and errors that may occur during web scraping operations, such as connection errors or missing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    element = soup.find('tag')\n",
    "    print(element.text)\n",
    "except AttributeError as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Extracted Data:\n",
    "Save the extracted data to a file or database for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write(title + '\\n')\n",
    "    f.write(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps provide a basic overview of working with Beautiful Soup for web scraping tasks. Customize the code examples based on the specific HTML structure and data you want to extract from web pages. Remember to follow ethical guidelines and respect website terms of service while scraping data from websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
